\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
% \usepackage[belowskip=-5pt,aboveskip=0.4pt]{caption} 
% \setlength{\textfloatsep}{0.5ex}
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[usenames, dvipsnames]{color}
\usepackage{soul}
\usepackage{cancel}
\usepackage{amsthm}
%\usepackage[draft]{hyperref}
\usepackage{hyperref}
\usepackage{diagbox} 
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{booktabs,caption}
\usepackage[flushleft]{threeparttable}
\usepackage[explicit]{titlesec}

\usepackage[lined,commentsnumbered,ruled,]{algorithm2e}


\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\green}[1]{{\color{ForestGreen} #1}}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\newtheorem{definition}{Definition}

%PDF Info Is Required:
  \pdfinfo{
/Title (2019 Formatting Instructions for Authors Using LaTeX)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Structure-Preserving Transformation: Generating Diverse and Transferable Adversarial Examples %that Keep Structure Patterns
}
\author{
}
\author{Dan Peng\textsuperscript{1},
Zizhan Zheng\textsuperscript{2},
Xiaofeng Zhang\textsuperscript{1}\\
\textsuperscript{1}{Harbin Institute of Technology (Shenzhen)}\\
\textsuperscript{2}{Tulane University, USA}\\
pengdan@stu.hit.edu.cn,
zzheng3@tulane.edu,
zhangxiaofeng@hit.edu.cn}
% %\title{The Water Mill}
\maketitle


%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Dan Peng\inst{1}
%Zizhan Zheng\inst{2}  \and
%Xiaofeng Zhang\inst{1} \\
%\institute{\inst{1}Harbin Institute of Technology \and \inst{2}Tulane University, USA
%}
%}
%

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

%
\maketitle
% \blue{
% \begin{itemize}
%     \item I am not sure which property of the adversaries is more suitable for mentioning in the title, "Natural" or "Diverse"? 
%     \red{Why the proposed approach is natural and why is it diverse? Think about how to justify each of them and choose the one that is easier to justify}
%     \item What do you think we call the transformation "quasi-isometric transformation"? isometric map means it keep distance or shape after mapping. In mathematics, quasi-isometry is an equivalence relation on metric spaces that ignores their small-scale details in favor of their coarse structure. \url{https://en.wikipedia.org/wiki/Quasi-isometry}
%     \red{We can use it if we can clearly define what the metric is in our setting and how the two properties mentioned in the wikipedia article are satisfied. Otherwise, it's better to choose a term that is more intuitive such as "structure-preserving transformation"}
%     \blue{nice! then let's call it structure-preserving transformation, but what abbreviation for it, "SPT"? }\red{OK to me.}
% \end{itemize}
% }


\begin{abstract}

Adversarial examples are perturbed inputs designed to fool machine learning models. Most recent works on adversarial examples for image classification focus on directly modifying pixels with minor perturbations. A common requirement in all these works is that the malicious perturbations should be small enough (measured by an $L_p$ norm for some $p$) so that they are imperceptible to humans. However, small perturbations can be unnecessarily restrictive and limit the diversity of adversarial examples generated. Further, an $L_p$ norm based distance metric ignores important structure patterns hidden in images that are important to human perception. Consequently, even the minor perturbation introduced in recent works often makes the adversarial examples less natural to humans. More importantly, they often do not transfer well and are therefore less effective when attacking black-box models especially for those protected by a defense mechanism. In this paper,  we propose a {\it structure-preserving transformation (SPT)} for generating natural and diverse adversarial examples with extremely high transferability. The key idea of our approach is to allow perceptible deviation in adversarial examples while keeping structure patterns that are central to a human classifier. Empirical results on the MNIST and the fashion-MNIST datasets show that adversarial examples generated by our approach can easily bypass strong adversarial training. Further, they transfer well to other target models with no loss or little loss of successful attack rate. 
%Adversarial examples are perturbed inputs designed to fool machine learning models. Recenxt works on adversarial examples for image classification either directly modify some image pixels with minor perturbation, or perturb latent representation on generative network. However, in all these works, the malicious perturbation must be small enough under the $L_{\infty}, L_{0}, L_{1} $ or $L_{2}$ metrics; And this make the existing approaches can only produce a class of adversarial examples, that is, which highly close to the original image.  That is not our expectancy, being an attacker, we are eager to generate as much as possible diverse and natural adversaries. More importantly, the adversaries generated by existing approaches are little transferable. 
 %Even the high attack success rate under white-box scenario, The low transferability 
 %which makes the existing attack methods less effective when attacking black-box models, especially for those protected by a defense mechanism. In fact, there is no need for the imperceptible perturbation to a clean image with the rapid advancement of research on this topic \cite{goodfellow2018defense}. For these reasons, We generalize this pursuit in a novel direction: Can we generate natural, diverse, while transferable adversaries without perturbation restriction? We proposed Histogram transformation (HT) for generating natural but diverse adversaries with extremely high transferability. 
 %We include experiments to show that the generated adversaries are natural, legible to humans, diverse sufficiently.
%We present generated adversaries to demonstrate the competitive potential of the proposed approach for white and black attack, whether with defense or not.Our empirical results on the MNIST, fashion-MNIST datasets show that generative adversarial examples can easily bypass strong adversarial training.
%  We show that HT achieves the \textbf{state of the art} attack on MNIST and fashion-MNIST datasets under black attack ( whether with or without defense) and white attack with defense, even it is not the most effective under white attack without defense. 
% We demonstrate the competitive potential of the proposed approach for white and black attack, whether with defense or not.
%  Finally, HT also successfully damages the current state of the art defense \cite{2017towardsmadry}on a public MNIST challenge(ignore small perturbation requirement), against whether black-box or white attack; In white-box attack, We drop the current lowest accuracy  \textbf{88.79\%} \textbf{to} \textbf{9.79\%} \cite{zheng2018distributionally}. In black-box attack,  We drop the current lowest accuracy  \textbf{92.79\%} \textbf{to} \textbf{9.80\%} \cite{xiao2018generating}.
%  \blue{modified, please check if it makes sense.}
 
 
%  However, for the former, the malicious perturbation are often noisy or unclear. Moreover, they are not transferable.
%  That is, these crafted  adversarial examples achieve a high successful rate only for a specific target model and is ineffective on other  models. At the same time, for a given target model, they generate one adversarial example for an given input clean image, which is not our expectation. In fact, for attacking, we should generate as much as possible natural adversarial image with strong transfer-ability. For the reasons, we propose an framework to generate natural and diverse adversarial examples that keep inherent structure of inputs, by conferring each structures with unique transformations. We present generated adversaries to demonstrate the potential of the proposed approach for white and black attack, whether with defense or not. We include experiments to show that the generated adversaries are natural, legible to humans ,diverse sufficiently and able to transfer across different models almost without loss of successful attack rate.

\end{abstract} 
\input{intro.tex}
\input{background.tex}
\input{methodology.tex}
\input{evaluation.tex}
\input{related.tex}
\section{Conclusion and Further Work}
In this paper, we propose the technique of structure-preserving transformation (SPT) to generate natural and diverse adversarial examples with high transferability. %from original images beyond small perturbation requirement. 
SPT keeps the semantic similarity between adversarial examples and original images by preserving the structure patterns between them. This new approach allows the ground truth of original images to be shared with adversarial examples without imposing the small perturbation requirement.   
Empirical results on the MNIST and fashion-MNIST datasets show that SPT can generate adversarial examples that are natural to humans while being sufficiently diverse.  
Further, the adversarial examples significantly reduce the classification accuracy of target models %to very low accuracy
whether defenses are applied or not. In particular, we show that SPT can easily bypass PGD-based adversarial training. Moreover, SPT adversarial examples can transfer across different models with little or no loss of successful attack rates. The high successful attack rates and outstanding transferability stem from the key property that SPT adversarial examples follow different distributions from the training data of target models. 
% Finally, we show that SPT can easily bypass the PGD adversarial training in the public MNIST challenge, reducing the accuracy of the white-box network to 9.79\% and the black-box network to 9.80\%.
% from the best result in learder board with 88.79\% and dropping the accuracy of black-box networks to 9.80\% while the best result is 92.76\%.

Although we focus on untargeted attacks and gray-scale images in the paper, the core idea of SPT can extend to targeted attacks and color images.  In our preliminary experiments for color images, the high successful attack rate is at the expense of images being less  discriminable and authentic.
How to ensure the quality of images %under the premise of 
while maintaining a high successful attack rate for color images is an important challenge that we will investigate in our future work. In addition, we will test the performance of SPT-based adversarial training and study the performance of SPT attacks under other defense methods that do not rely on $\ell_p$ attacks.  

%In this paper, we focus on untargeted attacks and gray-scale images. However, the core idea of SPT readily extends to targeted attacks and color images, which will be investigated in our future work.





%\bigskip
%\noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!
% \clearpage
% \newpage
%\red{check references. make sure the format is consistent. replace CoRR by arXiv link} 
%\blue{corrected, please check if they make sense.}
%\newpage{~~}
%\newpage{}
\bibliography{ref}
\bibliographystyle{aaai}
\newpage{~~}
\newpage{}
 \input{appendix.tex}
\end{document}







